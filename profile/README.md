<div align="center" style="margin-bottom:1em">
  <h1 align="center">GSO: Challenging Software Optimization Tasks for Evaluating SWE-Agents</h1>
  
  <div>GSO (Global Software Optimization) is a benchmark of over 100 optimization tasks across codebases and languages.</div>
  <div>Agents are tasked to optimize software against precise performance tests and are judged against expert developer commits.</div>
</div>
<hr/>


This organization contains the source code for the GSO benchmark, including:
* [GSO](https://github.com/gso-bench/gso), a benchmark for evaluating AI systems on real world GitHub issues.
* [Experiments](https://github.com/gso-bench/gso-experiments), execution logs, trajectories, and results from evaluation runs on GSO.
* [Example Usage](https://github.com/gso-bench/OpenHands/tree/gso-eval/evaluation/benchmarks/gso), an example of running GSO on your Agent (in this case OpenHands) to generate GSO solutions.
